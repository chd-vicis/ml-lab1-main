{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1 - Scikit-learn\n",
    "Author: Christopher DiMattia\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The goal of this lab is to become familiar with the scikit-learn library.\n",
    "\n",
    "You will practice loading example datasets, perform classification and regression with linear scikit-learn models, and investigate the effects of reducing the number of features (columns in X) and the number of samples (rows in X and y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "Using yellowbrick spam - classification  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "The goal is to investigate `LogisticRegression(max_iter=2000)` and effects of reducing the number of features and number of samples on classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_classifier_accuracy(model, X, y):\n",
    "    '''Calculate train and validation accuracy of classifier (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn classifier): Classifier to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training accuracy, validation accuracy\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #split data into training/validaiton\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=956)\n",
    "\n",
    "    #fit model\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    #predict on training dataa\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    #compare predicted to training data to get training accuracy\n",
    "    accuracy_train = accuracy_score(y_train, y_train_predict)\n",
    "\n",
    "    #predict on validation data\n",
    "    y_val_predict = model.predict(X_val)\n",
    "    #compare predicted to validation data to get validation accuracy\n",
    "    accuracy_val = accuracy_score(y_val,y_val_predict)\n",
    "\n",
    "    #return training and validation accuracies\n",
    "    return(accuracy_train, accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_spam()`, load the spam data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print size and type of `X` and `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size: 262200.  X types: float & int\n",
      "y size: 4600.  Y type is boolean (spam or not spam represented as 0 or 1)\n",
      "X dimensions: (4600, 57)\n",
      "y dimensions: (4600,)\n",
      "X types word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "Y type int64\n"
     ]
    }
   ],
   "source": [
    "from yellowbrick.datasets.loaders import load_spam\n",
    "\n",
    "#I'm not sure if you wanted the actual size function or the dimensions so I put both.  I also don't know what is meant by type (I assume data type).\n",
    "X, y = load_spam()\n",
    "print(\"X size: \" + str(X.size) + \".  X types: float & int\")\n",
    "print(\"y size: \" + str(y.size) + \".  Y type is boolean (spam or not spam represented as 0 or 1)\")\n",
    "print(\"X dimensions: \" + str(X.shape))\n",
    "print(\"y dimensions: \" + str(y.shape))\n",
    "print(\"X types \" + str(X.dtypes))\n",
    "print(\"Y type \" + str(y.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_small size: 2622.  X types: float & int\n",
      "y_small size: 46.  Y type is boolean (spam or not spam represented as 0 or 1)\n",
      "X_small dimensions: (46, 57)\n",
      "y_small dimensions: (46,)\n",
      "X_small types word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "Y_small type int64\n"
     ]
    }
   ],
   "source": [
    "X_small, X_val, y_small, y_val = train_test_split(X, y, random_state=174, train_size=0.01)\n",
    "print(\"X_small size: \" + str(X_small.size) + \".  X types: float & int\")\n",
    "print(\"y_small size: \" + str(y_small.size) + \".  Y type is boolean (spam or not spam represented as 0 or 1)\")\n",
    "print(\"X_small dimensions: \" + str(X_small.shape))\n",
    "print(\"y_small dimensions: \" + str(y_small.shape))\n",
    "print(\"X_small types \" + str(X_small.dtypes))\n",
    "print(\"Y_small type \" + str(y_small.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train and evaluate models\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "4. Call your convenience function `get_classifier_accuracy()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation accuracy for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSize</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>TRain Acc minus Val Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X &amp; y</td>\n",
       "      <td>0.935072</td>\n",
       "      <td>0.917391</td>\n",
       "      <td>0.017681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X &amp; y w/ 2 features</td>\n",
       "      <td>0.608986</td>\n",
       "      <td>0.613043</td>\n",
       "      <td>-0.004058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X &amp; y, small</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.191176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DataSize  Training Accuracy  Validation Accuracy  \\\n",
       "0                X & y           0.935072             0.917391   \n",
       "1  X & y w/ 2 features           0.608986             0.613043   \n",
       "2         X & y, small           0.941176             0.750000   \n",
       "\n",
       "   TRain Acc minus Val Acc  \n",
       "0                 0.017681  \n",
       "1                -0.004058  \n",
       "2                 0.191176  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#create model\n",
    "logRegression = LogisticRegression(max_iter=2000)\n",
    "\n",
    "#get accuracies useing the convenience function\n",
    "X_y = get_classifier_accuracy(logRegression,X,y)\n",
    "X_y_2col = get_classifier_accuracy(logRegression,X.iloc[:,0:2],y)\n",
    "X_y_small = get_classifier_accuracy(logRegression,X_small,y_small)\n",
    "\n",
    "#add labels for each datasize and include difference in accuracy between validation and training to help answer next question\n",
    "X_y = (\"X & y\"),X_y[0],X_y[1],X_y[0]-X_y[1]\n",
    "X_y_2col = (\"X & y w/ 2 features\"),X_y_2col[0],X_y_2col[1],X_y_2col[0]-X_y_2col[1]\n",
    "X_y_small = (\"X & y, small\"),X_y_small[0],X_y_small[1],X_y_small[0]-X_y_small[1]\n",
    "\n",
    "#list of tuples to input into dataframe\n",
    "data = [X_y,X_y_2col,X_y_small]\n",
    "\n",
    "#create dataframe and fill with results\n",
    "results = pd.DataFrame(data, columns=[\"DataSize\",\"Training Accuracy\",\"Validation Accuracy\",\"TRain Acc minus Val Acc\"])\n",
    "#print/show results\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Questions\n",
    "1. What is the validation accuracy using all data? What is the difference between training and validation accuracy?\n",
    "2. How does the validation accuracy and difference between training and validation change when only two columns are used? Provide values.\n",
    "3. How does the validation accuracy and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "Answer\n",
    "1. Validation accuracy with all data: 91.7%.  Training accuracy is 1.8% higher\n",
    "2. The validation accuracy drops drastically to 61.3& (a 30.4% drop) but the difference between the training and validation accuracy is still relatively small at 0%\n",
    "3. The validation accuracy greatly drops from 91.7% to 75% but the difference between the training and validation accuracy drastically increases to a 19.1% difference\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression\n",
    "\n",
    "Using yellowbrick energy - regression  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/energy.html\n",
    "\n",
    "The goal is to investigate `LinearRegression()` and effects of reducing the number of features and number of samples on regression performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_regressor_mse(model, X, y):\n",
    "    '''Calculate train and validation mean-squared error (mse) of regressor (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn regressor): Regressor to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training mse, validation mse\n",
    "    \n",
    "    '''\n",
    "    #same as above\n",
    "    #split data into training/validaiton\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=956)\n",
    "\n",
    "    #fit model\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    #predict on training dataa\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    #same as classification except this time it's regression so using mse accuracy to get accuracy\n",
    "    mse_train = mean_squared_error(y_train, y_train_predict)\n",
    "\n",
    "    #predict on validation data\n",
    "    y_val_predict = model.predict(X_val)\n",
    "    #compare predicted to validation data to get mse\n",
    "    mse_val = mean_squared_error(y_val,y_val_predict)\n",
    "\n",
    "    #return training and validation accuracies\n",
    "    return(mse_train, mse_val)\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_energy()` load the energy data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print dimensions and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size: 6144.  X types: float & int\n",
      "y size: 768.  Y type is boolean (spam or not spam represented as 0 or 1)\n",
      "X dimensions: (768, 8)\n",
      "y dimensions: (768,)\n",
      "X types relative compactness         float64\n",
      "surface area                 float64\n",
      "wall area                    float64\n",
      "roof area                    float64\n",
      "overall height               float64\n",
      "orientation                    int64\n",
      "glazing area                 float64\n",
      "glazing area distribution      int64\n",
      "dtype: object\n",
      "Y type float64\n"
     ]
    }
   ],
   "source": [
    "from yellowbrick.datasets.loaders import load_energy\n",
    "\n",
    "#Similar to above\n",
    "X, y = load_energy()\n",
    "print(\"X size: \" + str(X.size) + \".  X types: float & int\")\n",
    "print(\"y size: \" + str(y.size) + \".  Y type is boolean (spam or not spam represented as 0 or 1)\")\n",
    "print(\"X dimensions: \" + str(X.shape))\n",
    "print(\"y dimensions: \" + str(y.shape))\n",
    "print(\"X types \" + str(X.dtypes))\n",
    "print(\"Y type \" + str(y.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_small size: 56.  X types: float & int\n",
      "y_small size: 7.  Y type is boolean (spam or not spam represented as 0 or 1)\n",
      "X_small dimensions: (7, 8)\n",
      "y_small dimensions: (7,)\n",
      "X_small types relative compactness         float64\n",
      "surface area                 float64\n",
      "wall area                    float64\n",
      "roof area                    float64\n",
      "overall height               float64\n",
      "orientation                    int64\n",
      "glazing area                 float64\n",
      "glazing area distribution      int64\n",
      "dtype: object\n",
      "Y_small type float64\n"
     ]
    }
   ],
   "source": [
    "#Same as above.  Split the data into training and validation sets\n",
    "X_small, X_val, y_small, y_val = train_test_split(X, y, random_state=174, train_size=0.01)\n",
    "print(\"X_small size: \" + str(X_small.size) + \".  X types: float & int\")\n",
    "print(\"y_small size: \" + str(y_small.size) + \".  Y type is boolean (spam or not spam represented as 0 or 1)\")\n",
    "print(\"X_small dimensions: \" + str(X_small.shape))\n",
    "print(\"y_small dimensions: \" + str(y_small.shape))\n",
    "print(\"X_small types \" + str(X_small.dtypes))\n",
    "print(\"Y_small type \" + str(y_small.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train and evaluate models\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training MSE, validation MSE\n",
    "4. Call your convenience function `get_regressor_mse()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation MSE for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSize</th>\n",
       "      <th>Training MSE</th>\n",
       "      <th>Validation MSE</th>\n",
       "      <th>Train MSE minus Val MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X &amp; y</td>\n",
       "      <td>7.981975e+00</td>\n",
       "      <td>10.292306</td>\n",
       "      <td>-2.310331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X &amp; y w/ 2 features</td>\n",
       "      <td>5.360043e+01</td>\n",
       "      <td>46.410426</td>\n",
       "      <td>7.190004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X &amp; y, small</td>\n",
       "      <td>2.284541e-28</td>\n",
       "      <td>69.977449</td>\n",
       "      <td>-69.977449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DataSize  Training MSE  Validation MSE  Train MSE minus Val MSE\n",
       "0                X & y  7.981975e+00       10.292306                -2.310331\n",
       "1  X & y w/ 2 features  5.360043e+01       46.410426                 7.190004\n",
       "2         X & y, small  2.284541e-28       69.977449               -69.977449"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#create model\n",
    "linearRegression = LinearRegression()\n",
    "\n",
    "#get accuracies useing the convenience function\n",
    "X_y = get_regressor_mse(linearRegression,X,y)\n",
    "X_y_2col = get_regressor_mse(linearRegression,X.iloc[:,0:2],y)\n",
    "X_y_small = get_regressor_mse(linearRegression,X_small,y_small)\n",
    "\n",
    "X_y\n",
    "\n",
    "#add labels for each datasize and include difference in accuracy between validation and training to help answer next question\n",
    "X_y = (\"X & y\"),X_y[0],X_y[1],X_y[0]-X_y[1]\n",
    "X_y_2col = (\"X & y w/ 2 features\"),X_y_2col[0],X_y_2col[1],X_y_2col[0]-X_y_2col[1]\n",
    "X_y_small = (\"X & y, small\"),X_y_small[0],X_y_small[1],X_y_small[0]-X_y_small[1]\n",
    "\n",
    "#list of tuples to input into dataframe\n",
    "data = [X_y,X_y_2col,X_y_small]\n",
    "\n",
    "#create dataframe and fill with results\n",
    "results = pd.DataFrame(data, columns=[\"DataSize\",\"Training MSE\",\"Validation MSE\",\"Train MSE minus Val MSE\"])\n",
    "#print/show results\n",
    "results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Questions\n",
    "1. What is the validation MSE using all data? What is the difference between training and validation MSE?\n",
    "1. How does the validation MSE and difference between training and validation change when only two columns are used? Provide values.\n",
    "1. How does the validation MSE and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. The validation MSE for all data is 10.3.  The difference between the two is -2.3 (higher validation MSE)\n",
    "2. When only two columns are used the validation MSE increases (gets worse) from 10.3 to 46.4 while the difference in validation MSE and training MSE becomes larger at 7.2 (training  minus validation).\n",
    "3. The validation MSE greatly increases from 10.3 to 70.0 while the difference in validation and training MSE drastically increases to -70.0 (training minus validation).\n",
    "\n",
    "See above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observations/Interpretation\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "Patterns:\n",
    "1. More features obviously results in high accuracies (compare 92% vs 67% for classification and 10.3 vs 46.4 mse for regression) because the model is able to generalize better because it can utalize all the information from the features.\n",
    "2. Larger data sets also obviously result in higher accuracies (compare 92% vs 75% for classification and 10.3 vs 70.0 for regression) and the inverse is true.  With a smaller dataset the model doesn't have enough information to generalize properly.\n",
    "3. Smaller data sets will still retain high training accuracy () because even with a small amount of data a curve can still be fit quite well (we can conceive of an extreme example of only a few data points that can be fit perfectly).  In comparision removing most/all of a datas features will result in a low training accuracy which makes sense given that two features could be highly uncorrelated or even antagonistic to each other.  With so few features the model is not able to fully generalize as it is missing key information from other features so validation accuracy is clearly going to be poor.  Unless those 2 specific features are overwhelmingly impactful the model is likely to have poor performance as most models attempt to generalize complex infomation and relationships.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflection\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "Likes\n",
    "1. I greatly liked the ease of which I could create and get results from the models given how long the actual mathematical implmentation is.  I particularily like how the way sklearn is set up in that I am able to swap out any models with ease to compare different ML model fits.\n",
    "2. I liked having a large dataset that was non-trivial.  While trivial datasets are excellent for learning it's a nice change of pace to use a larger amount of data.  It made me realize that interpretting results will be far more challenging than with simple assignments and while this assignment was not heavily focused on interpretation it does make it clear that interpreting results with many features will pose a challenge.\n",
    "\n",
    "Dislikes\n",
    "1. Not a dislike about the assignment in particular but due to how other assignments lined up this week I did not have a chance to test other models just to see for comparision.  I want to do more comparisions between different types of models to get a better handle of the advantages/disadvantages of each via direct comparision.\n",
    "2. While I liked the ease with which the models are implemented I do think it would be beneficial to create a model from scratch so as to better appreciate the math behind the model.  I also think it would be interesting to do a \"speed\" comparision between these highly optimized models and a laymen's implmentation.\n",
    "3. I didn't get a sense for the weights that were applied to each feature.  Had I had more time I would have liked to print those (I assume I can) and try to see if I could identify or at least get a grasp of what features most impact the results.\n",
    "4. I'm not sure it would be useful but perhaps more plotting of results would be intesting for interpretation (to see how useful/useless it is).  That said I can do this on my own.\n",
    "\n",
    "Interesting, Confusing, etc\n",
    "1.  I found the assignment fairly straight forward with little confusion or challenge.  The most difficult part was the interpretation as that required the most critical thought and I'm still not sure I captured/know all the main theoretical points.  I for one struggle to conceptualize how models perform when the features/data sizes are manipulated beyong the obvious conclusions (Alot is obvious such as \"more data is obviously better\", but certain queestions I don't have a solid answer for.  An example: Can you create two different models that have similar performance and the exact same data but completely different weighting/interpretations?  I'm sure there are methods to address this but I haven't looked into them).  I also found it very interesting and motivating that the accuracy for the spam data was so high (91% with all data) given how complex natural langauge is, espcially given these are probably the simplest ML techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "641f7f3cbb8671fe5a13a7fa70714512a233d1eb5cdd3c217fb9962a98f45e99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
