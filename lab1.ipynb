{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab1 - Scikit-learn\n",
    "Author: Christopher DiMattia\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The goal of this lab is to become familiar with the scikit-learn library.\n",
    "\n",
    "You will practice loading example datasets, perform classification and regression with linear scikit-learn models, and investigate the effects of reducing the number of features (columns in X) and the number of samples (rows in X and y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification\n",
    "\n",
    "Using yellowbrick spam - classification  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "The goal is to investigate `LogisticRegression(max_iter=2000)` and effects of reducing the number of features and number of samples on classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_classifier_accuracy(model, X, y):\n",
    "    '''Calculate train and validation accuracy of classifier (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn classifier): Classifier to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training accuracy, validation accuracy\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #split data into training/validaiton\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=956)\n",
    "\n",
    "    #fit model\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    #predict on training dataa\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    #compare predicted to training data to get training accuracy\n",
    "    accuracy_train = accuracy_score(y_train, y_train_predict)\n",
    "\n",
    "    #predict on validation data\n",
    "    y_val_predict = model.predict(X_val)\n",
    "    #compare predicted to validation data to get validation accuracy\n",
    "    accuracy_val = accuracy_score(y_val,y_val_predict)\n",
    "\n",
    "    #return training and validation accuracies\n",
    "    return(accuracy_train, accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_spam()`, load the spam data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print size and type of `X` and `y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size: 262200.  X types: float & int\n",
      "y size: 4600.  Y type is boolean (spam or not spam represented as 0 or 1)\n",
      "X dimensions: (4600, 57)\n",
      "y dimensions: (4600,)\n"
     ]
    }
   ],
   "source": [
    "from yellowbrick.datasets.loaders import load_spam\n",
    "\n",
    "#I'm not sure if you wanted the actual size function or the dimensions so I put both.  I also don't know what is meant by type so I simply stated\n",
    "# the data types?  I assume type is not referring to classification vs regression as this section is clearly classification.\n",
    "X, y = load_spam()\n",
    "print(\"X size: \" + str(X.size) + \".  X types: float & int\")\n",
    "print(\"y size: \" + str(y.size) + \".  Y type is boolean (spam or not spam represented as 0 or 1)\")\n",
    "print(\"X dimensions: \" + str(X.shape))\n",
    "print(\"y dimensions: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X small size: 2622\n",
      "y small size: 46\n"
     ]
    }
   ],
   "source": [
    "X_small, X_val, y_small, y_val = train_test_split(X, y, random_state=174, train_size=0.01)\n",
    "print(\"X small size: \" + str(X_small.size))\n",
    "print(\"y small size: \" + str(y_small.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train and evaluate models\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "4. Call your convenience function `get_classifier_accuracy()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation accuracy for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSize</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Val Acc minus Train Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X &amp; y</td>\n",
       "      <td>0.935072</td>\n",
       "      <td>0.917391</td>\n",
       "      <td>-0.017681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X &amp; y w/ 2 features</td>\n",
       "      <td>0.608986</td>\n",
       "      <td>0.613043</td>\n",
       "      <td>0.004058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X &amp; y, small</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.191176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DataSize  Training Accuracy  Validation Accuracy  \\\n",
       "0                X & y           0.935072             0.917391   \n",
       "1  X & y w/ 2 features           0.608986             0.613043   \n",
       "2         X & y, small           0.941176             0.750000   \n",
       "\n",
       "   Val Acc minus Train Acc  \n",
       "0                -0.017681  \n",
       "1                 0.004058  \n",
       "2                -0.191176  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#create model\n",
    "logRegression = LogisticRegression(max_iter=2000)\n",
    "\n",
    "#get accuracies useing the convenience function\n",
    "X_y = get_classifier_accuracy(logRegression,X,y)\n",
    "X_y_2col = get_classifier_accuracy(logRegression,X.iloc[:,0:2],y)\n",
    "X_y_small = get_classifier_accuracy(logRegression,X_small,y_small)\n",
    "\n",
    "#add labels for each datasize and include difference in accuracy between validation and training to help answer next question\n",
    "X_y = (\"X & y\"),X_y[0],X_y[1],X_y[1]-X_y[0]\n",
    "X_y_2col = (\"X & y w/ 2 features\"),X_y_2col[0],X_y_2col[1],X_y_2col[1]-X_y_2col[0]\n",
    "X_y_small = (\"X & y, small\"),X_y_small[0],X_y_small[1],X_y_small[1]-X_y_small[0]\n",
    "\n",
    "#list of tuples to input into dataframe\n",
    "data = [X_y,X_y_2col,X_y_small]\n",
    "\n",
    "#create dataframe and fill with results\n",
    "results = pd.DataFrame(data, columns=[\"DataSize\",\"Training Accuracy\",\"Validation Accuracy\",\"Val Acc minus Train Acc\"])\n",
    "#print/show results\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Questions\n",
    "1. What is the validation accuracy using all data? What is the difference between training and validation accuracy?\n",
    "2. How does the validation accuracy and difference between training and validation change when only two columns are used? Provide values.\n",
    "3. How does the validation accuracy and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "Answer\n",
    "1. Validation accuracy with all data: 91.7%.  Training accuracy is 1.8% higher\n",
    "2. The validation accuracy drops drastically to 61.3& (a 30.4% drop) but the difference between the training and validation accuracy is still relatively small at 0%\n",
    "3. The validation accuracy stays very similar at 94.1% (only a 0.6% increase) but the difference between the training and validation accuracy drastically increases to a 19.1% difference (with training accuracy being greater than validation accuracy.  Training: 94.1% & Validation: 75.0% accuracies)\n",
    "\n",
    "See above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression\n",
    "\n",
    "Using yellowbrick energy - regression  \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/energy.html\n",
    "\n",
    "The goal is to investigate `LinearRegression()` and effects of reducing the number of features and number of samples on regression performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implement convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_regressor_mse(model, X, y):\n",
    "    '''Calculate train and validation mean-squared error (mse) of regressor (model)\n",
    "        \n",
    "        Splits feature matrix X and target vector y \n",
    "        with sklearn train_test_split() and random_state=956.\n",
    "        \n",
    "        model (sklearn regressor): Regressor to train and evaluate\n",
    "        X (numpy.array or pandas.DataFrame): Feature matrix\n",
    "        y (numpy.array or pandas.Series): Target vector\n",
    "        \n",
    "        returns: training mse, validation mse\n",
    "    \n",
    "    '''\n",
    "    #same as above\n",
    "    #split data into training/validaiton\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=956)\n",
    "\n",
    "    #fit model\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    #predict on training dataa\n",
    "    y_train_predict = model.predict(X_train)\n",
    "    #same as classification except this time it's regression so using mse accuracy to get accuracy\n",
    "    mse_train = mean_squared_error(y_train, y_train_predict)\n",
    "\n",
    "    #predict on validation data\n",
    "    y_val_predict = model.predict(X_val)\n",
    "    #compare predicted to validation data to get mse\n",
    "    mse_val = mean_squared_error(y_val,y_val_predict)\n",
    "\n",
    "    #return training and validation accuracies\n",
    "    return(mse_train, mse_val)\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load data\n",
    "\n",
    "Use the yellowbrick function `load_energy()` load the energy data set into feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print dimensions and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions: (768, 8).  X types: real & int\n",
      "y dimensions: (768,).  y type is float\n"
     ]
    }
   ],
   "source": [
    "from yellowbrick.datasets.loaders import load_energy\n",
    "\n",
    "#Similar to above I assume the type refers to the data type of the feature matrix and target vector\n",
    "X, y = load_energy()\n",
    "print(\"X dimensions: \" + str(X.shape) + \".  X types: real & int\")\n",
    "print(\"y dimensions: \" + str(y.shape) + \".  y type is float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn function `train_test_split()` prepare a feature matrix `X_small` and target vector `y_small` that contain only **1%** of the rows. Use `random_state=174`.\n",
    "\n",
    "Print size and type of `X_small` and `y_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions: (7, 8).  X types: real & int\n",
      "y dimensions: (7,).  y type is float\n"
     ]
    }
   ],
   "source": [
    "#Same as above.  Split the data into training and validation sets\n",
    "X_small, X_val, y_small, y_val = train_test_split(X, y, random_state=174, train_size=0.01)\n",
    "print(\"X dimensions: \" + str(X_small.shape) + \".  X types: real & int\")\n",
    "print(\"y dimensions: \" + str(y_small.shape) + \".  y type is float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train and evaluate models\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Create a pandas DataFrame `results` with columns: Data size, training MSE, validation MSE\n",
    "4. Call your convenience function `get_regressor_mse()` using \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`\n",
    "5. Add the data size, training and validation MSE for each call to the `results` DataFrame\n",
    "6. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSize</th>\n",
       "      <th>Training MSE</th>\n",
       "      <th>Validation MSE</th>\n",
       "      <th>Val MSE minus Train MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X &amp; y</td>\n",
       "      <td>7.981975e+00</td>\n",
       "      <td>10.292306</td>\n",
       "      <td>2.310331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X &amp; y w/ 2 features</td>\n",
       "      <td>5.360043e+01</td>\n",
       "      <td>46.410426</td>\n",
       "      <td>-7.190004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X &amp; y, small</td>\n",
       "      <td>2.284541e-28</td>\n",
       "      <td>69.977449</td>\n",
       "      <td>69.977449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              DataSize  Training MSE  Validation MSE  Val MSE minus Train MSE\n",
       "0                X & y  7.981975e+00       10.292306                 2.310331\n",
       "1  X & y w/ 2 features  5.360043e+01       46.410426                -7.190004\n",
       "2         X & y, small  2.284541e-28       69.977449                69.977449"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#create model\n",
    "linearRegression = LinearRegression()\n",
    "\n",
    "#get accuracies useing the convenience function\n",
    "X_y = get_regressor_mse(linearRegression,X,y)\n",
    "X_y_2col = get_regressor_mse(linearRegression,X.iloc[:,0:2],y)\n",
    "X_y_small = get_regressor_mse(linearRegression,X_small,y_small)\n",
    "\n",
    "X_y\n",
    "\n",
    "#add labels for each datasize and include difference in accuracy between validation and training to help answer next question\n",
    "X_y = (\"X & y\"),X_y[0],X_y[1],X_y[1]-X_y[0]\n",
    "X_y_2col = (\"X & y w/ 2 features\"),X_y_2col[0],X_y_2col[1],X_y_2col[1]-X_y_2col[0]\n",
    "X_y_small = (\"X & y, small\"),X_y_small[0],X_y_small[1],X_y_small[1]-X_y_small[0]\n",
    "\n",
    "#list of tuples to input into dataframe\n",
    "data = [X_y,X_y_2col,X_y_small]\n",
    "\n",
    "#create dataframe and fill with results\n",
    "results = pd.DataFrame(data, columns=[\"DataSize\",\"Training MSE\",\"Validation MSE\",\"Val MSE minus Train MSE\"])\n",
    "#print/show results\n",
    "results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Questions\n",
    "1. What is the validation MSE using all data? What is the difference between training and validation MSE?\n",
    "1. How does the validation MSE and difference between training and validation change when only two columns are used? Provide values.\n",
    "1. How does the validation MSE and difference between training and validation change when only 1% of the rows are used? Provide values.\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. The MSE for all data is 8.0 and 10.3 for the training and validation data respectively.  The difference between the two is 2.3 (valdiation minus training)\n",
    "2. When only two columns are used the MSE decreases (gets better) slightly (from 8.0 to 5.4) while the validation accuracy becomes much worse (from 10.3 to 46.4).\n",
    "3. The training MSE greatly improves from 8.0 to 0.0 while the validation MSE greatly worsens from 10.3 to 70.0.\n",
    "\n",
    "See above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observations/Interpretation\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "Patterns:\n",
    "1. More features obviously results in high accuracies (compare 92% vs 67% for classification and 10.3 vs 46.4 mse for regression)\n",
    "2. Larger data sets also obviously result in higher accuracies (compare 92% vs 75% for classification and 10.3 vs 70.0 for regression)\n",
    "3. Smaller data sets will still retain high training accuracy because even with a small amount of data a curve can still be fit quite well (we can conceive of an extreme example of only a few data points that can be fit perfectly).  In comparision removing most/all of a datas features will result in a low training accuracy which makes sense given that two features could be highly uncorrelated or even antagonistic to each other.  With so few features they can appear more as \"outliers\" or noise given that they might only make a small contribution to the predictive accuracy of a model or fit.  Unless those 2 specific features are the most impactful it's more likely to look like noise and thus have a lower accuracy and for any complex set of data this will not be the case by definition.\n",
    "4. Another patter is that the accuracy of the training vs validation accuracy for removed features is very similar.  Again this makes sense because removing features basically makes the fit \"fuzzy\" but it doesn't misrepresent the data, whereas removing data can lead to a misrepresentation of the data.  By taking only 1% of the avaliable data, bias and chances for outliers greatly increases which is why the training vs validation accuracy is so different between the smaller and large sets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reflection\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "Likes\n",
    "1. I greatly liked the ease of which I could create and get results from the models given how long the actual mathematical implmentation is.  I particularily like how the way sklearn is set up in that I am able to swap out any models with ease to compare different ML model fits.\n",
    "2. I liked having a large dataset that was non-trivial.  While trivial datasets are excellent for learning it's a nice change of pace to use a larger amount of data.  It made me realize that interpretting results will be far more challenging than with simple assignments and while this assignment was not heavily focused on interpretation it does make it clear that interpreting results with many features will pose a challenge.\n",
    "\n",
    "Dislikes\n",
    "1. Not a dislike about the assignment in particular but due to how other assignments lined up this week I did not have a chance to test other models just to see for comparision.  I want to do more comparisions between different types of models to get a better handle of the advantages/disadvantages of each via direct comparision.\n",
    "2. While I liked the ease with which the models are implemented I do think it would be beneficial to create a model from scratch so as to better appreciate the math behind the model.  I also think it would be interesting to do a \"speed\" comparision between these highly optimized models and a laymen's implmentation.\n",
    "3. I didn't get a sense for the weights that were applied to each feature.  Had I had more time I would have liked to print those and try to see if I could identify or at least get a grasp of what features most impact the results.\n",
    "4. I'm not sure it would be useful but perhaps more plotting of results would be intesting for interpretation (to see how useful/useless it is).  That said I can do this on my own.\n",
    "\n",
    "Interesting, Confusing, etc\n",
    "1.  I found the assignment fairly straight forward with little confusion or challenge.  The most difficult part was the interpretation as that required the most critical thought and I'm still not sure I captured/know all the main theoretical points.   I also found it very interesting and motivating that the accuracy for the spam data was so high (91% with all data) given how complex natural langauge is, espcially given these are probably the simplest ML techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "641f7f3cbb8671fe5a13a7fa70714512a233d1eb5cdd3c217fb9962a98f45e99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
